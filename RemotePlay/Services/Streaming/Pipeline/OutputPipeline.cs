using System;
using System.Threading;
using System.Threading.Channels;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using RemotePlay.Models.PlayStation;
using RemotePlay.Models.Streaming;
using RemotePlay.Services.Streaming.Receiver;
using RemotePlay.Services.Streaming.Protocol;

namespace RemotePlay.Services.Streaming.Pipeline
{
    /// <summary>
    /// Output Pipeline - è´Ÿè´£å¼‚æ­¥å‘é€å¸§åˆ° WebRTC Receiver
    /// è®¾è®¡ç›®æ ‡ï¼š
    /// 1. å®Œå…¨å¼‚æ­¥ï¼ˆä¸é˜»å¡ä¸Šæ¸¸ Pipelineï¼‰
    /// 2. ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆIDR å…³é”®å¸§ä¼˜å…ˆå‘é€ï¼‰
    /// 3. èƒŒå‹ä¿æŠ¤ï¼ˆé˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæ—§å¸§ï¼‰
    /// 4. æ€§èƒ½ç›‘æ§
    /// </summary>
    public sealed class OutputPipeline : IDisposable
    {
        private readonly ILogger<OutputPipeline> _logger;
        private volatile IAVReceiver _receiver;
        private readonly Channel<ProcessedFrame> _videoChannel;
        private readonly Channel<ProcessedFrame> _audioChannel;
        private readonly int _videoQueueCapacity;
        private readonly CancellationTokenSource _cts = new();
        private readonly Task _videoSendTask;
        private readonly Task _audioSendTask;

        // âœ… å¸§ç‡æ§åˆ¶ï¼šç›®æ ‡60fpsï¼Œå³æ¯å¸§é—´éš”çº¦16.67ms
        private const int TARGET_FPS = 60;
        private const double TARGET_FRAME_INTERVAL_MS = 1000.0 / TARGET_FPS; // ~16.67ms
        private DateTime _lastVideoFrameSentTime = DateTime.MinValue;

        // ç»Ÿè®¡
        private long _videoFramesSent;
        private long _audioFramesSent;
        private long _videoFramesDropped;
        private long _audioFramesDropped;
        private long _priorityFramesSent;

        public OutputPipeline(
            ILogger<OutputPipeline> logger,
            IAVReceiver receiver,
            int videoQueueCapacity = 256,
            int audioQueueCapacity = 512)
        {
            _logger = logger;
            _receiver = receiver;
            _videoQueueCapacity = videoQueueCapacity;

            // âœ… ä¼˜åŒ–ï¼šè§†é¢‘é˜Ÿåˆ—ä½¿ç”¨ DropOldest ç­–ç•¥ï¼Œä½†ä¼˜å…ˆä¿ç•™å…³é”®å¸§
            // å½“é˜Ÿåˆ—æ»¡æ—¶ï¼Œä¼˜å…ˆä¸¢å¼ƒéå…³é”®å¸§ï¼Œä¿ç•™å…³é”®å¸§
            _videoChannel = Channel.CreateBounded<ProcessedFrame>(new BoundedChannelOptions(videoQueueCapacity)
            {
                FullMode = BoundedChannelFullMode.DropOldest,
                SingleReader = true,
                SingleWriter = false
            });

            // éŸ³é¢‘é˜Ÿåˆ— - å®¹é‡æ›´å¤§ï¼Œä¼˜å…ˆä¿è¯éŸ³é¢‘è¿ç»­æ€§
            _audioChannel = Channel.CreateBounded<ProcessedFrame>(new BoundedChannelOptions(audioQueueCapacity)
            {
                FullMode = BoundedChannelFullMode.DropOldest,
                SingleReader = true,
                SingleWriter = false
            });

            // å¯åŠ¨ç‹¬ç«‹å‘é€çº¿ç¨‹
            _videoSendTask = Task.Run(VideoSendLoop, _cts.Token);
            _audioSendTask = Task.Run(AudioSendLoop, _cts.Token);
        }

        #region Public API

        /// <summary>
        /// æ¨é€è§†é¢‘å¸§ï¼ˆéé˜»å¡ï¼‰
        /// âœ… ä¼˜åŒ–ï¼šå½“é˜Ÿåˆ—æ»¡æ—¶ï¼Œä¼˜å…ˆä¸¢å¼ƒéå…³é”®å¸§ï¼Œä¿ç•™å…³é”®å¸§
        /// </summary>
        public bool TryPushVideoFrame(ProcessedFrame frame)
        {
            // âœ… å¦‚æœæ˜¯å…³é”®å¸§ä¸”é˜Ÿåˆ—æ¥è¿‘æ»¡ï¼Œå°è¯•å…ˆä¸¢å¼ƒä¸€ä¸ªéå…³é”®å¸§
            if (frame.IsKeyFrame && _videoChannel.Reader.Count >= _videoQueueCapacity * 0.8)
            {
                // å°è¯•è¯»å–å¹¶ä¸¢å¼ƒä¸€ä¸ªéå…³é”®å¸§
                if (_videoChannel.Reader.TryRead(out ProcessedFrame oldFrame))
                {
                    if (!oldFrame.IsKeyFrame)
                    {
                        Interlocked.Increment(ref _videoFramesDropped);
                        _logger.LogDebug("ğŸ” Output video queue: dropped non-keyframe to make room for keyframe");
                    }
                    else
                    {
                        // å¦‚æœæ˜¯å…³é”®å¸§ï¼Œæ”¾å›å»
                        _videoChannel.Writer.TryWrite(oldFrame);
                    }
                }
            }
            
            bool success = _videoChannel.Writer.TryWrite(frame);
            if (!success)
            {
                Interlocked.Increment(ref _videoFramesDropped);
                // âœ… å¦‚æœæ˜¯å…³é”®å¸§ï¼Œè®°å½•è­¦å‘Šï¼›å¦‚æœæ˜¯æ™®é€šå¸§ï¼Œåªè®°å½•è°ƒè¯•ä¿¡æ¯
                if (frame.IsKeyFrame)
                {
                    _logger.LogWarning("âš ï¸ Output video queue full, dropping keyframe={Frame}", frame.FrameIndex);
                }
                else
                {
                    _logger.LogDebug("âš ï¸ Output video queue full, dropping frame={Frame}", frame.FrameIndex);
                }
            }
            return success;
        }

        /// <summary>
        /// æ¨é€éŸ³é¢‘å¸§ï¼ˆéé˜»å¡ï¼‰
        /// </summary>
        public bool TryPushAudioFrame(ProcessedFrame frame)
        {
            bool success = _audioChannel.Writer.TryWrite(frame);
            if (!success)
            {
                Interlocked.Increment(ref _audioFramesDropped);
                _logger.LogWarning("âš ï¸ Output audio queue full, dropping frame={Frame}", frame.FrameIndex);
            }
            return success;
        }

        /// <summary>
        /// è®¾ç½®æ¥æ”¶å™¨ï¼ˆæ”¯æŒåŠ¨æ€åˆ‡æ¢ï¼Œä¾‹å¦‚ä» DefaultReceiver åˆ‡æ¢åˆ° WebRTCReceiverï¼‰
        /// </summary>
        public void SetReceiver(IAVReceiver receiver)
        {
            if (receiver == null)
            {
                _logger.LogWarning("âš ï¸ SetReceiver: receiver is null");
                return;
            }
            
            var oldReceiver = _receiver?.GetType().Name ?? "null";
            _receiver = receiver;
            _logger.LogInformation("âœ… OutputPipeline: Receiver switched from {Old} to {New}", 
                oldReceiver, receiver.GetType().Name);
        }

        /// <summary>
        /// è·å–ç»Ÿè®¡ä¿¡æ¯
        /// </summary>
        public OutputStats GetStats()
        {
            return new OutputStats
            {
                VideoFramesSent = Interlocked.Read(ref _videoFramesSent),
                AudioFramesSent = Interlocked.Read(ref _audioFramesSent),
                VideoFramesDropped = Interlocked.Read(ref _videoFramesDropped),
                AudioFramesDropped = Interlocked.Read(ref _audioFramesDropped),
                PriorityFramesSent = Interlocked.Read(ref _priorityFramesSent),
                VideoQueueSize = _videoChannel.Reader.Count,
                AudioQueueSize = _audioChannel.Reader.Count
            };
        }

        #endregion

        #region Send Loops

        private async Task VideoSendLoop()
        {
            _logger.LogInformation("âœ… OutputPipeline video sender started");

            try
            {
                long receivedCount = 0;
                _lastVideoFrameSentTime = DateTime.UtcNow;
                
                await foreach (var frame in _videoChannel.Reader.ReadAllAsync(_cts.Token))
                {
                    receivedCount++;
                    try
                    {
                        // âœ… æ¸¸æˆä¸²æµä¼˜åŒ–ï¼šä¼˜å…ˆä½å»¶è¿Ÿï¼Œåªåœ¨æç«¯æƒ…å†µä¸‹æ‰é™åˆ¶å¸§ç‡
                        var now = DateTime.UtcNow;
                        if (_lastVideoFrameSentTime != DateTime.MinValue)
                        {
                            var elapsed = (now - _lastVideoFrameSentTime).TotalMilliseconds;
                            var queueSize = _videoChannel.Reader.Count;
                            
                            // âœ… æ¸¸æˆä¸²æµï¼šä¼˜å…ˆä½å»¶è¿Ÿï¼Œåªåœ¨å‘é€è¿‡å¿«æ—¶æ‰è½»å¾®é™åˆ¶
                            // âœ… å…³é”®å¸§å’Œç§¯å‹ä¸¥é‡æ—¶ï¼šç«‹å³å‘é€ï¼Œä¸å»¶è¿Ÿ
                            if (frame.IsKeyFrame || queueSize >= 20)
                            {
                                // å…³é”®å¸§æˆ–é˜Ÿåˆ—ç§¯å‹ï¼šç«‹å³å‘é€ï¼Œä¿è¯ä½å»¶è¿Ÿ
                                // ä¸å»¶è¿Ÿ
                            }
                            else if (!frame.IsKeyFrame && queueSize < 20)
                            {
                                // âœ… æ­£å¸¸æƒ…å†µï¼šåªåœ¨å‘é€è¿‡å¿«ï¼ˆ< 8msï¼Œå³>125fpsï¼‰æ—¶æ‰è½»å¾®å»¶è¿Ÿï¼Œé¿å…æç«¯æƒ…å†µ
                                // æ¸¸æˆä¸²æµå…è®¸æ›´é«˜çš„å¸§ç‡æ³¢åŠ¨ï¼Œä¼˜å…ˆä¿è¯ä½å»¶è¿Ÿ
                                var minInterval = 8.0; // æœ€å°é—´éš”8msï¼ˆæœ€å¤§125fpsï¼‰ï¼Œåªåœ¨æç«¯æƒ…å†µä¸‹é™åˆ¶
                                if (elapsed < minInterval)
                                {
                                    var delayMs = minInterval - elapsed;
                                    await Task.Delay(TimeSpan.FromMilliseconds(delayMs), _cts.Token);
                                    now = DateTime.UtcNow;
                                }
                            }
                        }
                        _lastVideoFrameSentTime = now;

                        // æ„å»ºå¸¦ header çš„åŒ…
                        var packetData = new byte[1 + frame.Data.Length];
                        packetData[0] = (byte)HeaderType.VIDEO;
                        Array.Copy(frame.Data, 0, packetData, 1, frame.Data.Length);

                        // âš ï¸ è°ƒè¯•ï¼šè®°å½•å‘é€çš„å¸§ä¿¡æ¯ï¼ˆä½¿ç”¨ Information çº§åˆ«ï¼Œç¡®ä¿ä¸ä¼šè¢«è¿‡æ»¤ï¼‰
                        if (receivedCount % 100 == 0 || frame.IsKeyFrame)
                        {
                            var queueSize = _videoChannel.Reader.Count;
                            _logger.LogDebug("ğŸ” OutputPipeline: Sending video frame={Frame}, isKeyFrame={Key}, dataLen={Len}, receiver={Receiver}, received={Received}, queueSize={Queue}",
                                frame.FrameIndex, frame.IsKeyFrame, packetData.Length, _receiver?.GetType().Name ?? "null", receivedCount, queueSize);
                        }

                        // æ ¹æ®æ˜¯å¦ä¸ºå…³é”®å¸§é€‰æ‹©å‘é€æ–¹å¼
                        if (frame.IsKeyFrame && _receiver is WebRTCReceiver webrtcReceiver)
                        {
                            webrtcReceiver.OnVideoPacketPriority(packetData);
                            Interlocked.Increment(ref _priorityFramesSent);
                        }
                        else
                        {
                            _receiver.OnVideoPacket(packetData);
                        }

                        Interlocked.Increment(ref _videoFramesSent);
                    }
                    catch (Exception ex)
                    {
                        _logger.LogError(ex, "âŒ Video send error, frame={Frame}", frame.FrameIndex);
                    }
                }
                _logger.LogDebug("ğŸ” OutputPipeline VideoSendLoop: Total received {Count} frames, sent {Sent} frames", 
                    receivedCount, Interlocked.Read(ref _videoFramesSent));
            }
            catch (OperationCanceledException)
            {
                // æ­£å¸¸é€€å‡º
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "âŒ VideoSendLoop exception");
            }
            finally
            {
                _logger.LogInformation("âœ… OutputPipeline video sender exited");
            }
        }

        private async Task AudioSendLoop()
        {
            _logger.LogInformation("âœ… OutputPipeline audio sender started");

            try
            {
                await foreach (var frame in _audioChannel.Reader.ReadAllAsync(_cts.Token))
                {
                    try
                    {
                        // æ„å»ºå¸¦ header çš„åŒ…
                        var packetData = new byte[1 + frame.Data.Length];
                        packetData[0] = (byte)HeaderType.AUDIO;
                        Array.Copy(frame.Data, 0, packetData, 1, frame.Data.Length);

                        _receiver.OnAudioPacket(packetData);
                        Interlocked.Increment(ref _audioFramesSent);
                    }
                    catch (Exception ex)
                    {
                        _logger.LogError(ex, "âŒ Audio send error, frame={Frame}", frame.FrameIndex);
                    }
                }
            }
            catch (OperationCanceledException)
            {
                // æ­£å¸¸é€€å‡º
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "âŒ AudioSendLoop exception");
            }
            finally
            {
                _logger.LogInformation("âœ… OutputPipeline audio sender exited");
            }
        }

        #endregion

        #region Dispose

        public void Dispose()
        {
            _videoChannel.Writer.Complete();
            _audioChannel.Writer.Complete();
            _cts.Cancel();

            try
            {
                Task.WaitAll(new[] { _videoSendTask, _audioSendTask }, TimeSpan.FromMilliseconds(500));
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex, "âš ï¸ OutputPipeline dispose error");
            }

            _cts.Dispose();
        }

        #endregion
    }

    /// <summary>
    /// Output Pipeline ç»Ÿè®¡ä¿¡æ¯
    /// </summary>
    public struct OutputStats
    {
        public long VideoFramesSent { get; set; }
        public long AudioFramesSent { get; set; }
        public long VideoFramesDropped { get; set; }
        public long AudioFramesDropped { get; set; }
        public long PriorityFramesSent { get; set; }
        public int VideoQueueSize { get; set; }
        public int AudioQueueSize { get; set; }
    }
}


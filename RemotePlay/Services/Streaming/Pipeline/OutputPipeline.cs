using System;
using System.Threading;
using System.Threading.Channels;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using RemotePlay.Models.PlayStation;
using RemotePlay.Models.Streaming;
using RemotePlay.Services.Streaming.Receiver;
using RemotePlay.Services.Streaming.Protocol;

namespace RemotePlay.Services.Streaming.Pipeline
{
    /// <summary>
    /// Output Pipeline - è´Ÿè´£å¼‚æ­¥å‘é€å¸§åˆ° WebRTC Receiver
    /// è®¾è®¡ç›®æ ‡ï¼š
    /// 1. å®Œå…¨å¼‚æ­¥ï¼ˆä¸é˜»å¡ä¸Šæ¸¸ Pipelineï¼‰
    /// 2. ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆIDR å…³é”®å¸§ä¼˜å…ˆå‘é€ï¼‰
    /// 3. èƒŒå‹ä¿æŠ¤ï¼ˆé˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæ—§å¸§ï¼‰
    /// 4. æ€§èƒ½ç›‘æ§
    /// </summary>
    public sealed class OutputPipeline : IDisposable
    {
        private readonly ILogger<OutputPipeline> _logger;
        private volatile IAVReceiver _receiver;  // âš ï¸ ä¿®å¤ï¼šä½¿ç”¨ volatile æ”¯æŒåŠ¨æ€åˆ‡æ¢ receiver
        private readonly Channel<ProcessedFrame> _videoChannel;
        private readonly Channel<ProcessedFrame> _audioChannel;
        private readonly CancellationTokenSource _cts = new();
        private readonly Task _videoSendTask;
        private readonly Task _audioSendTask;

        // ç»Ÿè®¡
        private long _videoFramesSent;
        private long _audioFramesSent;
        private long _videoFramesDropped;
        private long _audioFramesDropped;
        private long _priorityFramesSent;

        public OutputPipeline(
            ILogger<OutputPipeline> logger,
            IAVReceiver receiver,
            int videoQueueCapacity = 256,
            int audioQueueCapacity = 512)
        {
            _logger = logger;
            _receiver = receiver;

            // è§†é¢‘é˜Ÿåˆ— - ä½¿ç”¨ DropOldest ç­–ç•¥ä¿è¯æœ€æ–°å¸§ä¼˜å…ˆ
            _videoChannel = Channel.CreateBounded<ProcessedFrame>(new BoundedChannelOptions(videoQueueCapacity)
            {
                FullMode = BoundedChannelFullMode.DropOldest,
                SingleReader = true,
                SingleWriter = false
            });

            // éŸ³é¢‘é˜Ÿåˆ— - å®¹é‡æ›´å¤§ï¼Œä¼˜å…ˆä¿è¯éŸ³é¢‘è¿ç»­æ€§
            _audioChannel = Channel.CreateBounded<ProcessedFrame>(new BoundedChannelOptions(audioQueueCapacity)
            {
                FullMode = BoundedChannelFullMode.DropOldest,
                SingleReader = true,
                SingleWriter = false
            });

            // å¯åŠ¨ç‹¬ç«‹å‘é€çº¿ç¨‹
            _videoSendTask = Task.Run(VideoSendLoop, _cts.Token);
            _audioSendTask = Task.Run(AudioSendLoop, _cts.Token);
        }

        #region Public API

        /// <summary>
        /// æ¨é€è§†é¢‘å¸§ï¼ˆéé˜»å¡ï¼‰
        /// </summary>
        public bool TryPushVideoFrame(ProcessedFrame frame)
        {
            bool success = _videoChannel.Writer.TryWrite(frame);
            if (!success)
            {
                Interlocked.Increment(ref _videoFramesDropped);
                _logger.LogWarning("âš ï¸ Output video queue full, dropping frame={Frame}", frame.FrameIndex);
            }
            return success;
        }

        /// <summary>
        /// æ¨é€éŸ³é¢‘å¸§ï¼ˆéé˜»å¡ï¼‰
        /// </summary>
        public bool TryPushAudioFrame(ProcessedFrame frame)
        {
            bool success = _audioChannel.Writer.TryWrite(frame);
            if (!success)
            {
                Interlocked.Increment(ref _audioFramesDropped);
                _logger.LogWarning("âš ï¸ Output audio queue full, dropping frame={Frame}", frame.FrameIndex);
            }
            return success;
        }

        /// <summary>
        /// è®¾ç½®æ¥æ”¶å™¨ï¼ˆæ”¯æŒåŠ¨æ€åˆ‡æ¢ï¼Œä¾‹å¦‚ä» DefaultReceiver åˆ‡æ¢åˆ° WebRTCReceiverï¼‰
        /// </summary>
        public void SetReceiver(IAVReceiver receiver)
        {
            if (receiver == null)
            {
                _logger.LogWarning("âš ï¸ SetReceiver: receiver is null");
                return;
            }
            
            var oldReceiver = _receiver?.GetType().Name ?? "null";
            _receiver = receiver;
            _logger.LogInformation("âœ… OutputPipeline: Receiver switched from {Old} to {New}", 
                oldReceiver, receiver.GetType().Name);
        }

        /// <summary>
        /// è·å–ç»Ÿè®¡ä¿¡æ¯
        /// </summary>
        public OutputStats GetStats()
        {
            return new OutputStats
            {
                VideoFramesSent = Interlocked.Read(ref _videoFramesSent),
                AudioFramesSent = Interlocked.Read(ref _audioFramesSent),
                VideoFramesDropped = Interlocked.Read(ref _videoFramesDropped),
                AudioFramesDropped = Interlocked.Read(ref _audioFramesDropped),
                PriorityFramesSent = Interlocked.Read(ref _priorityFramesSent),
                VideoQueueSize = _videoChannel.Reader.Count,
                AudioQueueSize = _audioChannel.Reader.Count
            };
        }

        #endregion

        #region Send Loops

        private async Task VideoSendLoop()
        {
            _logger.LogInformation("âœ… OutputPipeline video sender started");

            try
            {
                long receivedCount = 0;
                await foreach (var frame in _videoChannel.Reader.ReadAllAsync(_cts.Token))
                {
                    receivedCount++;
                    try
                    {
                        // æ„å»ºå¸¦ header çš„åŒ…
                        var packetData = new byte[1 + frame.Data.Length];
                        packetData[0] = (byte)HeaderType.VIDEO;
                        Array.Copy(frame.Data, 0, packetData, 1, frame.Data.Length);

                        // âš ï¸ è°ƒè¯•ï¼šè®°å½•å‘é€çš„å¸§ä¿¡æ¯ï¼ˆä½¿ç”¨ Information çº§åˆ«ï¼Œç¡®ä¿ä¸ä¼šè¢«è¿‡æ»¤ï¼‰
                        if (receivedCount % 100 == 0 || frame.IsKeyFrame)
                        {
                            _logger.LogDebug("ğŸ” OutputPipeline: Sending video frame={Frame}, isKeyFrame={Key}, dataLen={Len}, receiver={Receiver}, received={Received}",
                                frame.FrameIndex, frame.IsKeyFrame, packetData.Length, _receiver?.GetType().Name ?? "null", receivedCount);
                        }

                        // æ ¹æ®æ˜¯å¦ä¸ºå…³é”®å¸§é€‰æ‹©å‘é€æ–¹å¼
                        if (frame.IsKeyFrame && _receiver is WebRTCReceiver webrtcReceiver)
                        {
                            webrtcReceiver.OnVideoPacketPriority(packetData);
                            Interlocked.Increment(ref _priorityFramesSent);
                        }
                        else
                        {
                            _receiver.OnVideoPacket(packetData);
                        }

                        Interlocked.Increment(ref _videoFramesSent);
                    }
                    catch (Exception ex)
                    {
                        _logger.LogError(ex, "âŒ Video send error, frame={Frame}", frame.FrameIndex);
                    }
                }
                _logger.LogDebug("ğŸ” OutputPipeline VideoSendLoop: Total received {Count} frames, sent {Sent} frames", 
                    receivedCount, Interlocked.Read(ref _videoFramesSent));
            }
            catch (OperationCanceledException)
            {
                // æ­£å¸¸é€€å‡º
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "âŒ VideoSendLoop exception");
            }
            finally
            {
                _logger.LogInformation("âœ… OutputPipeline video sender exited");
            }
        }

        private async Task AudioSendLoop()
        {
            _logger.LogInformation("âœ… OutputPipeline audio sender started");

            try
            {
                await foreach (var frame in _audioChannel.Reader.ReadAllAsync(_cts.Token))
                {
                    try
                    {
                        // æ„å»ºå¸¦ header çš„åŒ…
                        var packetData = new byte[1 + frame.Data.Length];
                        packetData[0] = (byte)HeaderType.AUDIO;
                        Array.Copy(frame.Data, 0, packetData, 1, frame.Data.Length);

                        _receiver.OnAudioPacket(packetData);
                        Interlocked.Increment(ref _audioFramesSent);
                    }
                    catch (Exception ex)
                    {
                        _logger.LogError(ex, "âŒ Audio send error, frame={Frame}", frame.FrameIndex);
                    }
                }
            }
            catch (OperationCanceledException)
            {
                // æ­£å¸¸é€€å‡º
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "âŒ AudioSendLoop exception");
            }
            finally
            {
                _logger.LogInformation("âœ… OutputPipeline audio sender exited");
            }
        }

        #endregion

        #region Dispose

        public void Dispose()
        {
            _videoChannel.Writer.Complete();
            _audioChannel.Writer.Complete();
            _cts.Cancel();

            try
            {
                Task.WaitAll(new[] { _videoSendTask, _audioSendTask }, TimeSpan.FromMilliseconds(500));
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex, "âš ï¸ OutputPipeline dispose error");
            }

            _cts.Dispose();
        }

        #endregion
    }

    /// <summary>
    /// Output Pipeline ç»Ÿè®¡ä¿¡æ¯
    /// </summary>
    public struct OutputStats
    {
        public long VideoFramesSent { get; set; }
        public long AudioFramesSent { get; set; }
        public long VideoFramesDropped { get; set; }
        public long AudioFramesDropped { get; set; }
        public long PriorityFramesSent { get; set; }
        public int VideoQueueSize { get; set; }
        public int AudioQueueSize { get; set; }
    }
}

